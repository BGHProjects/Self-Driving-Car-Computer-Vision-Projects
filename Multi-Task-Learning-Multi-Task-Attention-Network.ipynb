{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Task Learning | Multi Task Attention Network\n",
    "- The following is code for an application of a Multi Task Attention Network to perform Multi Task Learning on the CityScapes dataset, specifically to discern relevant features from the image and determine the depth in the image for the purposes of a self-driving car\n",
    "- The code includes very minor refactoring, but is majoritively based off of this Kaggle notebook (https://www.kaggle.com/code/sakshaymahna/mtan-multi-task-attention-network/notebook) as part of this tutorial (https://www.youtube.com/watch?v=cPOtULagNnI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.utils.data.sampler as sampler\n",
    "\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityScapes(Dataset):\n",
    "    def __init__(self, root, train=True):\n",
    "        self.train = train\n",
    "        self.root = os.path.expanduser(root)\n",
    "\n",
    "        # read the data file\n",
    "        if train:\n",
    "            self.data_path = root + '/train'\n",
    "        else:\n",
    "            self.data_path = root + '/val'\n",
    "\n",
    "        # calculate data length\n",
    "        self.data_len = len(fnmatch.filter(os.listdir(self.data_path + '/image'), '*.npy'))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load data from the pre-processed npy files\n",
    "        image = torch.from_numpy(np.moveaxis(np.load(self.data_path + '/image/{:d}.npy'.format(index)), -1, 0))\n",
    "        semantic = torch.from_numpy(np.load(self.data_path + '/label/{:d}.npy'.format(index)))\n",
    "        depth = torch.from_numpy(np.moveaxis(np.load(self.data_path + '/depth/{:d}.npy'.format(index)), -1, 0))\n",
    "        \n",
    "        return image.float(), semantic.float(), depth.float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utility Functions\n",
    "Define Task Metrics, Loss Functions, and Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(x_pred, x_output, task_type):\n",
    "    device = x_pred.device\n",
    "\n",
    "    # binary mark to mask out undefined pixel space\n",
    "    binary_mask = (torch.sum(x_output, dim=1) != 0).float().unsqueeze(1).to(device)\n",
    "\n",
    "    if task_type == 'semantic':\n",
    "        # semantic loss: depth-wise cross entropy\n",
    "        loss = F.nll_loss(x_pred, x_output, ignore_index=-1)\n",
    "\n",
    "    if task_type == 'depth':\n",
    "        # depth loss: l1 norm\n",
    "        loss = torch.sum(torch.abs(x_pred - x_output) * binary_mask) / torch.nonzero(binary_mask, as_tuple=False).size(0)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# mIoU and Acc. formula: accumulate every pixel and average across all pixels in all images\n",
    "class ConfMatrix(object):\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        self.mat = None\n",
    "\n",
    "    def update(self, pred, target):\n",
    "        n = self.num_classes\n",
    "        if self.mat is None:\n",
    "            self.mat = torch.zeros((n, n), dtype=torch.int64, device=pred.device)\n",
    "        with torch.no_grad():\n",
    "            k = (target >= 0) & (target < n)\n",
    "            inds = n * target[k].to(torch.int64) + pred[k]\n",
    "            self.mat += torch.bincount(inds, minlength=n ** 2).reshape(n, n)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        h = self.mat.float()\n",
    "        acc = torch.diag(h).sum() / h.sum()\n",
    "        iu = torch.diag(h) / (h.sum(1) + h.sum(0) - torch.diag(h))\n",
    "        return torch.mean(iu), acc\n",
    "    \n",
    "\n",
    "def depth_error(x_pred, x_output):\n",
    "    device = x_pred.device\n",
    "    binary_mask = (torch.sum(x_output, dim=1) != 0).unsqueeze(1).to(device)\n",
    "    x_pred_true = x_pred.masked_select(binary_mask)\n",
    "    x_output_true = x_output.masked_select(binary_mask)\n",
    "    abs_err = torch.abs(x_pred_true - x_output_true)\n",
    "    rel_err = torch.abs(x_pred_true - x_output_true) / x_output_true\n",
    "    return (torch.sum(abs_err) / torch.nonzero(binary_mask, as_tuple=False).size(0)).item(), \\\n",
    "           (torch.sum(rel_err) / torch.nonzero(binary_mask, as_tuple=False).size(0)).item()\n",
    "\n",
    "\n",
    "def multi_task_trainer(train_loader, test_loader, multi_task_model, device, optimizer, scheduler, config, total_epoch=200):\n",
    "    train_batch = len(train_loader)\n",
    "    test_batch = len(test_loader)\n",
    "    \n",
    "    T = config['temp']\n",
    "    avg_cost = np.zeros([total_epoch, 12], dtype=np.float32)\n",
    "    lambda_weight = np.ones([2, total_epoch])\n",
    "    \n",
    "    for index in range(total_epoch):\n",
    "        cost = np.zeros(12, dtype=np.float32)\n",
    "\n",
    "        # apply Dynamic Weight Average\n",
    "        if config['weight'] == 'dwa':\n",
    "            if index == 0 or index == 1:\n",
    "                lambda_weight[:, index] = 1.0\n",
    "            else:\n",
    "                w_1 = avg_cost[index - 1, 0] / avg_cost[index - 2, 0]\n",
    "                w_2 = avg_cost[index - 1, 3] / avg_cost[index - 2, 3]\n",
    "                lambda_weight[0, index] = 2 * np.exp(w_1 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T))\n",
    "                lambda_weight[1, index] = 2 * np.exp(w_2 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T))\n",
    "\n",
    "        # iteration for all batches\n",
    "        multi_task_model.train()\n",
    "        train_dataset = iter(train_loader)\n",
    "        conf_mat = ConfMatrix(multi_task_model.class_nb)\n",
    "        for k in range(train_batch):\n",
    "            train_data, train_label, train_depth = train_dataset.next()\n",
    "            train_data, train_label = train_data.to(device), train_label.long().to(device)\n",
    "            train_depth = train_depth.to(device)\n",
    "\n",
    "            train_pred, logsigma = multi_task_model(train_data)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_loss = [model_fit(train_pred[0], train_label, 'semantic'),\n",
    "                          model_fit(train_pred[1], train_depth, 'depth')]\n",
    "\n",
    "            if config['weight'] == 'equal' or config['weight'] == 'dwa':\n",
    "                loss = sum([lambda_weight[i, index] * train_loss[i] for i in range(2)])\n",
    "            else:\n",
    "                loss = sum(1 / (2 * torch.exp(logsigma[i])) * train_loss[i] + logsigma[i] / 2 for i in range(2))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # accumulate label prediction for every pixel in training images\n",
    "            conf_mat.update(train_pred[0].argmax(1).flatten(), train_label.flatten())\n",
    "\n",
    "            cost[0] = train_loss[0].item()\n",
    "            cost[3] = train_loss[1].item()\n",
    "            cost[4], cost[5] = depth_error(train_pred[1], train_depth)\n",
    "            avg_cost[index, :6] += cost[:6] / train_batch\n",
    "\n",
    "        # compute mIoU and acc\n",
    "        avg_cost[index, 1:3] = conf_mat.get_metrics()\n",
    "\n",
    "        # evaluating test data\n",
    "        multi_task_model.eval()\n",
    "        conf_mat = ConfMatrix(multi_task_model.class_nb)\n",
    "        with torch.no_grad():  # operations inside don't track history\n",
    "            test_dataset = iter(test_loader)\n",
    "            for k in range(test_batch):\n",
    "                test_data, test_label, test_depth = test_dataset.next()\n",
    "                test_data, test_label = test_data.to(device), test_label.long().to(device)\n",
    "                test_depth = test_depth.to(device)\n",
    "\n",
    "                test_pred, _ = multi_task_model(test_data)\n",
    "                test_loss = [model_fit(test_pred[0], test_label, 'semantic'),\n",
    "                             model_fit(test_pred[1], test_depth, 'depth')]\n",
    "\n",
    "                conf_mat.update(test_pred[0].argmax(1).flatten(), test_label.flatten())\n",
    "\n",
    "                cost[6] = test_loss[0].item()\n",
    "                cost[9] = test_loss[1].item()\n",
    "                cost[10], cost[11] = depth_error(test_pred[1], test_depth)\n",
    "                avg_cost[index, 6:] += cost[6:] / test_batch\n",
    "\n",
    "            # compute mIoU and acc\n",
    "            avg_cost[index, 7:9] = conf_mat.get_metrics()\n",
    "\n",
    "        scheduler.step()\n",
    "        print('Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} ||'\n",
    "            'TEST: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} '\n",
    "            .format(index, avg_cost[index, 0], avg_cost[index, 1], avg_cost[index, 2], avg_cost[index, 3],\n",
    "                    avg_cost[index, 4], avg_cost[index, 5], avg_cost[index, 6], avg_cost[index, 7], avg_cost[index, 8],\n",
    "                    avg_cost[index, 9], avg_cost[index, 10], avg_cost[index, 11]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Network \n",
    "Define the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegNet, self).__init__()\n",
    "        \n",
    "        # initialise network parameters\n",
    "        filter = [64, 128, 256, 512, 512]\n",
    "        self.class_nb = 19\n",
    "\n",
    "        # define encoder decoder layers\n",
    "        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n",
    "        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n",
    "        for i in range(4):\n",
    "            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n",
    "            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n",
    "\n",
    "        # define convolution layer\n",
    "        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n",
    "        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n",
    "        for i in range(4):\n",
    "            if i == 0:\n",
    "                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n",
    "                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n",
    "            else:\n",
    "                self.conv_block_enc.append(nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n",
    "                                                         self.conv_layer([filter[i + 1], filter[i + 1]])))\n",
    "                self.conv_block_dec.append(nn.Sequential(self.conv_layer([filter[i], filter[i]]),\n",
    "                                                         self.conv_layer([filter[i], filter[i]])))\n",
    "\n",
    "        # define task attention layers\n",
    "        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n",
    "        self.decoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])])])\n",
    "        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n",
    "        self.decoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n",
    "\n",
    "        for j in range(2):\n",
    "            if j < 2:\n",
    "                self.encoder_att.append(nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])]))\n",
    "                self.decoder_att.append(nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])]))\n",
    "            for i in range(4):\n",
    "                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n",
    "                self.decoder_att[j].append(self.att_layer([filter[i + 1] + filter[i], filter[i], filter[i]]))\n",
    "\n",
    "        for i in range(4):\n",
    "            if i < 3:\n",
    "                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n",
    "                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i]]))\n",
    "            else:\n",
    "                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n",
    "                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n",
    "\n",
    "        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], pred=True)\n",
    "        self.pred_task2 = self.conv_layer([filter[0], 1], pred=True)\n",
    "\n",
    "        # define pooling and unpooling functions\n",
    "        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
    "        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def conv_layer(self, channel, pred=False):\n",
    "        if not pred:\n",
    "            conv_block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(num_features=channel[1]),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            conv_block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n",
    "                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n",
    "            )\n",
    "        return conv_block\n",
    "\n",
    "    def att_layer(self, channel):\n",
    "        att_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(channel[1]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(channel[2]),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        return att_block\n",
    "\n",
    "    def forward(self, x):\n",
    "        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n",
    "        for i in range(5):\n",
    "            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n",
    "\n",
    "        # define attention list for tasks\n",
    "        atten_encoder, atten_decoder = ([0] * 3 for _ in range(2))\n",
    "        for i in range(2):\n",
    "            atten_encoder[i], atten_decoder[i] = ([0] * 5 for _ in range(2))\n",
    "        for i in range(2):\n",
    "            for j in range(5):\n",
    "                atten_encoder[i][j], atten_decoder[i][j] = ([0] * 3 for _ in range(2))\n",
    "\n",
    "        # define global shared network\n",
    "        for i in range(5):\n",
    "            if i == 0:\n",
    "                g_encoder[i][0] = self.encoder_block[i](x)\n",
    "                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n",
    "                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n",
    "            else:\n",
    "                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n",
    "                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n",
    "                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n",
    "\n",
    "        for i in range(5):\n",
    "            if i == 0:\n",
    "                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n",
    "                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n",
    "                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n",
    "            else:\n",
    "                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n",
    "                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n",
    "                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n",
    "\n",
    "        # define task dependent attention module\n",
    "        for i in range(2):\n",
    "            for j in range(5):\n",
    "                if j == 0:\n",
    "                    atten_encoder[i][j][0] = self.encoder_att[i][j](g_encoder[j][0])\n",
    "                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n",
    "                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n",
    "                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n",
    "                else:\n",
    "                    atten_encoder[i][j][0] = self.encoder_att[i][j](torch.cat((g_encoder[j][0], atten_encoder[i][j - 1][2]), dim=1))\n",
    "                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n",
    "                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n",
    "                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n",
    "\n",
    "            for j in range(5):\n",
    "                if j == 0:\n",
    "                    atten_decoder[i][j][0] = F.interpolate(atten_encoder[i][-1][-1], scale_factor=2, mode='bilinear', align_corners=True)\n",
    "                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n",
    "                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat((g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n",
    "                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n",
    "                else:\n",
    "                    atten_decoder[i][j][0] = F.interpolate(atten_decoder[i][j - 1][2], scale_factor=2, mode='bilinear', align_corners=True)\n",
    "                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n",
    "                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat((g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n",
    "                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n",
    "\n",
    "        # define task prediction layers\n",
    "        t1_pred = F.log_softmax(self.pred_task1(atten_decoder[0][-1][-1]), dim=1)\n",
    "        t2_pred = self.pred_task2(atten_decoder[1][-1][-1])\n",
    "\n",
    "        return [t1_pred, t2_pred], self.logsigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = {\n",
    "    'temp': 2.0,\n",
    "    'weight': 'dwa'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SegNet_MTAN = SegNet().to(device)\n",
    "optimizer = optim.Adam(SegNet_MTAN.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "print('LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR <11.25 <22.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../input/cityscapes-depth-and-segmentation/data'\n",
    "train_set = CityScapes(root=dataset_path, train=True)\n",
    "test_set = CityScapes(root=dataset_path, train=False)\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "               dataset=train_set,\n",
    "               batch_size=batch_size,\n",
    "               shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "              dataset=test_set,\n",
    "              batch_size=batch_size,\n",
    "              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_task_trainer(train_loader,\n",
    "                   test_loader,\n",
    "                   SegNet_MTAN,\n",
    "                   device,\n",
    "                   optimizer,\n",
    "                   scheduler,\n",
    "                   config,\n",
    "                   100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_DIR = '../input/cityscapes-depth-and-segmentation/data/val/'\n",
    "n_files = 10\n",
    "files = random.sample(os.listdir(VALID_DIR + 'image'), n_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentations = []\n",
    "depths = []\n",
    "\n",
    "for file in files:\n",
    "    img = torch.from_numpy(np.expand_dims(np.moveaxis(np.load(VALID_DIR + 'image/' + file), -1, 0), axis = 0))\n",
    "    img = img.to(device)\n",
    "    prediction, _ = SegNet_MTAN(img.float())\n",
    "    \n",
    "    seg_prediction = np.moveaxis(prediction[0][0].cpu().detach().numpy(), 0, -1).argmax(2)\n",
    "    depth_prediction = np.moveaxis(prediction[1][0].cpu().detach().numpy(), 0, -1)\n",
    "    \n",
    "    segmentations.append(seg_prediction)\n",
    "    depths.append(depth_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, seg_pred in zip(files, segmentations):\n",
    "    seg_label = np.load(VALID_DIR + 'label/' + file)\n",
    "    \n",
    "    plt.figure(figsize = (20,10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(seg_label)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(seg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, seg_pred in zip(files, segmentations):\n",
    "    img = np.load(VALID_DIR + 'image/' + file)\n",
    "    \n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(seg_pred, alpha = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, depth_pred in zip(files, depths):\n",
    "    depth_label = np.load(VALID_DIR + 'depth/' + file)\n",
    "    \n",
    "    plt.figure(figsize = (20,10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(depth_label)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(depth_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, depth_pred in zip(files, depths):\n",
    "    img = np.load(VALID_DIR + 'image/' + file)\n",
    "    \n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(depth_pred, alpha = 0.7)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
