{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera to Bird's Eye View | UNetXST\n",
    "- The following is code for an application of the UNetXST model to transform camera feed data into a bird's eye view representation for a self-driving car\n",
    "- The code includes very minor refactoring, but is majoritively based off of this Kaggle notebook (https://www.kaggle.com/code/sakshaymahna/unetxst/notebook) as part of this tutorial (https://www.youtube.com/watch?v=cPOtULagNnI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from easydict import EasyDict as edict\n",
    "from tensorflow.keras.layers import Layer, Input, Activation, MaxPooling2D, Flatten, Conv2D, Dense, Multiply, Conv2DTranspose, BatchNormalization, Dropout, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_in_folder(folder):\n",
    "    return sorted([os.path.join(folder, f) for f in os.listdir(folder)])\n",
    "\n",
    "def sample_list(*ls, n_samples, replace=False):\n",
    "    n_samples = min(len(ls[0]), n_samples)\n",
    "    idcs = np.random.choice(np.arange(0, len(ls[0])), n_samples, replace=replace)\n",
    "    samples = zip([np.take(l, idcs) for l in ls])\n",
    "    return samples, idcs\n",
    "\n",
    "def load_image(filename):\n",
    "    img = cv2.imread(filename)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "def load_image_op(filename):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    return img\n",
    "\n",
    "def resize_image(img, shape, interpolation=cv2.INTER_CUBIC):\n",
    "    # resize relevant image axis to length of corresponding target axis while preserving aspect ratio\n",
    "    axis = 0 if float(shape[0]) / float(img.shape[0]) > float(shape[1]) / float(img.shape[1]) else 1\n",
    "    factor = float(shape[axis]) / float(img.shape[axis])\n",
    "    img = cv2.resize(img, (0,0), fx=factor, fy=factor, interpolation=interpolation)\n",
    "\n",
    "    # crop other image axis to match target shape\n",
    "    center = img.shape[int(not axis)] / 2.0\n",
    "    step = shape[int(not axis)] / 2.0\n",
    "    left = int(center-step)\n",
    "    right = int(center+step)\n",
    "    if axis == 0:\n",
    "        img = img[:, left:right]\n",
    "    else:\n",
    "        img = img[left:right, :]\n",
    "\n",
    "    return img\n",
    "\n",
    "def resize_image_op(img, fromShape, toShape, cropToPreserveAspectRatio=True, interpolation=tf.image.ResizeMethod.BICUBIC):\n",
    "    if not cropToPreserveAspectRatio:\n",
    "        img = tf.image.resize(img, toShape, method=interpolation)\n",
    "\n",
    "    else:\n",
    "        # first crop to match target aspect ratio\n",
    "        fx = toShape[1] / fromShape[1]\n",
    "        fy = toShape[0] / fromShape[0]\n",
    "        relevantAxis = 0 if fx < fy else 1\n",
    "        if relevantAxis == 0:\n",
    "            crop = fromShape[0] * toShape[1] / toShape[0]\n",
    "            img = tf.image.crop_to_bounding_box(img, 0, int((fromShape[1] - crop) / 2), fromShape[0], int(crop))\n",
    "        else:\n",
    "            crop = fromShape[1] * toShape[0] / toShape[1]\n",
    "            img = tf.image.crop_to_bounding_box(img, int((fromShape[0] - crop) / 2), 0, int(crop), fromShape[1])\n",
    "\n",
    "        # then resize to target shape\n",
    "        img = tf.image.resize(img, toShape, method=interpolation)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def one_hot_encode_image(image, palette):\n",
    "    one_hot_map = []\n",
    "\n",
    "    # find instances of class colors and append layer to one-hot-map\n",
    "    for class_colors in palette:\n",
    "        class_map = np.zeros(image.shape[0:2], dtype=bool)\n",
    "        for color in class_colors:\n",
    "            class_map = class_map | (image == color).all(axis=-1)\n",
    "        one_hot_map.append(class_map)\n",
    "\n",
    "    # finalize one-hot-map\n",
    "    one_hot_map = np.stack(one_hot_map, axis=-1)\n",
    "    one_hot_map = one_hot_map.astype(np.float32)\n",
    "\n",
    "    return one_hot_map\n",
    "\n",
    "\n",
    "def one_hot_encode_image_op(image, palette):\n",
    "    one_hot_map = []\n",
    "\n",
    "    for class_colors in palette:\n",
    "\n",
    "        class_map = tf.zeros(image.shape[0:2], dtype=tf.int32)\n",
    "\n",
    "        for color in class_colors:\n",
    "            # find instances of color and append layer to one-hot-map\n",
    "            class_map = tf.bitwise.bitwise_or(class_map, tf.cast(tf.reduce_all(tf.equal(image, color), axis=-1), tf.int32))\n",
    "        one_hot_map.append(class_map)\n",
    "\n",
    "    # finalize one-hot-map\n",
    "    one_hot_map = tf.stack(one_hot_map, axis=-1)\n",
    "    one_hot_map = tf.cast(one_hot_map, tf.float32)\n",
    "\n",
    "    return one_hot_map\n",
    "\n",
    "\n",
    "def one_hot_decode_image(one_hot_image, palette):\n",
    "    # create empty image with correct dimensions\n",
    "    height, width = one_hot_image.shape[0:2]\n",
    "    depth = palette[0][0].size\n",
    "    image = np.zeros([height, width, depth])\n",
    "\n",
    "    # reduce all layers of one-hot-encoding to one layer with indices of the classes\n",
    "    map_of_classes = one_hot_image.argmax(2)\n",
    "\n",
    "    for idx, class_colors in enumerate(palette):\n",
    "        # fill image with corresponding class colors\n",
    "        image[np.where(map_of_classes == idx)] = class_colors[0]\n",
    "\n",
    "    image = image.astype(np.uint8)\n",
    "\n",
    "    return image\n",
    "\n",
    "def get_class_distribution(folder, shape, palette):\n",
    "    # get filepaths\n",
    "    files = [os.path.join(folder, f) for f in os.listdir(folder) if not f.startswith(\".\")]\n",
    "\n",
    "    n_classes = len(palette)\n",
    "\n",
    "    def get_img(file, shape, interpolation=cv2.INTER_NEAREST, one_hot_reduce=False):\n",
    "        img = load_image(file)\n",
    "        img = resize_image(img, shape, interpolation)\n",
    "        img = one_hot_encode_image(img, palette)\n",
    "        return img\n",
    "\n",
    "    px = shape[0] * shape[1]\n",
    "\n",
    "    distribution = {}\n",
    "    for k in range(n_classes):\n",
    "        distribution[str(k)] = 0\n",
    "\n",
    "    i = 0\n",
    "    bar = tqdm(files)\n",
    "    for f in bar:\n",
    "        img = get_img(f, shape)\n",
    "        classes = np.argmax(img, axis=-1)\n",
    "        unique, counts = np.unique(classes, return_counts=True)\n",
    "        occs = dict(zip(unique, counts))\n",
    "\n",
    "        for k in range(n_classes):\n",
    "            occ = occs[k] if k in occs.keys() else 0\n",
    "            distribution[str(k)] = (distribution[str(k)] * i + occ / px) / (i+1)\n",
    "\n",
    "        bar.set_postfix(distribution)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return distribution\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    def wcce(y_true, y_pred):\n",
    "        Kweights = tf.constant(weights)\n",
    "        if not tf.is_tensor(y_pred): y_pred = tf.constant(y_pred)\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        return tf.keras.backend.categorical_crossentropy(y_true, y_pred) * tf.keras.backend.sum(y_true * Kweights, axis=-1)\n",
    "\n",
    "    return wcce\n",
    "\n",
    "\n",
    "class MeanIoUWithOneHotLabels(tf.keras.metrics.MeanIoU):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.argmax(y_true, axis=-1)\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        return super().update_state(y_true, y_pred, sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../input/semantic-segmentation-bev/cam2bev-data-master-1_FRLR/1_FRLR/train/'\n",
    "VAL_PATH = '../input/semantic-segmentation-bev/cam2bev-data-master-1_FRLR/1_FRLR/val/'\n",
    "configs = {\n",
    "    \"input_training\": [TRAIN_PATH + 'front/front/', TRAIN_PATH + 'rear/rear/', TRAIN_PATH + 'left/left/', TRAIN_PATH + '/right/right/'],\n",
    "    \"label_training\": TRAIN_PATH + 'bev+occlusion/bev+occlusion/',\n",
    "    \"max_samples_training\": 10000,\n",
    "    \"input_validation\": [VAL_PATH + 'front/front/', VAL_PATH + 'rear/rear/', VAL_PATH + 'left/left/', VAL_PATH + '/right/right/'],\n",
    "    \"label_validation\": VAL_PATH + 'bev+occlusion/bev+occlusion/',\n",
    "    \"input_testing\": [VAL_PATH + 'front/front/', VAL_PATH + 'rear/rear/', VAL_PATH + 'left/left/', VAL_PATH + '/right/right/'],\n",
    "    \"label_testing\": VAL_PATH + 'bev+occlusion/bev+occlusion/',\n",
    "    \"max_samples_validation\": 10000,\n",
    "    \"max_samples_testing\": 10000,\n",
    "    \"image_shape\": [128, 256],\n",
    "    \"epochs\": 20,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"loss_weights\": [0.98684351, 2.2481491, 10.47452063, 4.78351389, 7.01028204, 8.41360361, 10.91633349, 2.38571558, 1.02473193, 2.79359197],\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"save_interval\": 5,\n",
    "    \"output_dir\": \"output\"\n",
    "}\n",
    "\n",
    "conf = edict(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palette Configuration for identifying classes based on colors\n",
    "conf.one_hot_palette_input = np.array([\n",
    "    np.array([[128, 64, 128]]),\n",
    "    np.array([[244, 35, 232], [250, 170, 160]]),\n",
    "    np.array([[255, 0, 0]]),\n",
    "    np.array([[0, 0, 142], [0, 0, 110]]),\n",
    "    np.array([[0, 0, 70]]),\n",
    "    np.array([[0, 60, 100], [0, 0, 90]]),\n",
    "    np.array([[220, 20, 60], [0, 0, 230], [119, 11, 32]]),\n",
    "    \n",
    "    np.array([[0, 0, 0], [111, 74, 0], [81, 0, 81], [230, 150, 140], [70, 70, 70],\n",
    "     [102, 102, 156], [190, 153, 153], [180, 165, 180], [150, 100, 100],\n",
    "     [150, 120, 90], [153, 153, 153], [153, 153, 153], [250, 170, 30],\n",
    "     [220, 220, 0], [0, 80, 100]]),\n",
    "    \n",
    "    np.array([[107, 142, 35], [152, 251, 152]]),\n",
    "    np.array([[70, 130, 180]])\n",
    "])\n",
    "\n",
    "conf.one_hot_palette_label = np.array([\n",
    "    np.array([[128, 64, 128]]),\n",
    "    np.array([[244, 35, 232], [250, 170, 160]]),\n",
    "    np.array([[255, 0, 0]]),\n",
    "    np.array([[0, 0, 142], [0, 0, 110]]),\n",
    "    np.array([[0, 0, 70]]),\n",
    "    np.array([[0, 60, 100], [0, 0, 90]]),\n",
    "    np.array([[220, 20, 60], [0, 0, 230], [119, 11, 32]]),\n",
    "    \n",
    "    np.array([[0, 0, 0], [111, 74, 0], [81, 0, 81], [230, 150, 140], [70, 70, 70],\n",
    "     [102, 102, 156], [190, 153, 153], [180, 165, 180], [150, 100, 100],\n",
    "     [150, 120, 90], [153, 153, 153], [153, 153, 153], [250, 170, 30],\n",
    "     [220, 220, 0], [0, 80, 100], [70, 130, 180]]),\n",
    "    \n",
    "    np.array([[107, 142, 35], [152, 251, 152]]),\n",
    "    np.array([[150, 150, 150]])\n",
    "])\n",
    "\n",
    "n_classes_input = len(conf.one_hot_palette_input)\n",
    "n_classes_label = len(conf.one_hot_palette_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homography Matrix for dataset\n",
    "H = [\n",
    "  np.array([[4.651574574230558e-14, 10.192351107009959, -5.36318723862984e-07], [-5.588661045867985e-07, 0.0, 2.3708767903941617], [35.30731833118676, 0.0, -1.7000018578614013]]),                                       # front\n",
    "  np.array([[-5.336674306912119e-14, -10.192351107009957, 5.363187220578325e-07], [5.588660952931949e-07, 3.582264351370481e-23, 2.370876772982613], [-35.30731833118661, -2.263156574813233e-15, -0.5999981421386035]]), # rear\n",
    "  np.array([[20.38470221401992, 7.562206982469407e-14, -0.28867638384075833], [-3.422067857504854e-23, 2.794330463189411e-07, 2.540225111648729], [2.1619497190382224e-15, -17.65365916559334, -0.4999990710692976]]),    # left\n",
    "  np.array([[-20.38470221401991, -4.849709834037436e-15, 0.2886763838407495], [-3.4220679184765114e-23, -2.794330512976549e-07, 2.5402251116487626], [2.161949719038217e-15, 17.653659165593304, -0.5000009289306967]])   # right\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max_samples_training random training samples\n",
    "n_inputs = len(conf.input_training)\n",
    "files_train_input = [get_files_in_folder(folder) for folder in conf.input_training]\n",
    "files_train_label = get_files_in_folder(conf.label_training)\n",
    "_, idcs = sample_list(files_train_label, n_samples=conf.max_samples_training)\n",
    "files_train_input = [np.take(f, idcs) for f in files_train_input]\n",
    "files_train_label = np.take(files_train_label, idcs)\n",
    "image_shape_original_input = load_image(files_train_input[0][0]).shape[0:2]\n",
    "image_shape_original_label = load_image(files_train_label[0]).shape[0:2]\n",
    "print(f\"Found {len(files_train_label)} training samples\")\n",
    "\n",
    "# get max_samples_validation random validation samples\n",
    "files_valid_input = [get_files_in_folder(folder) for folder in conf.input_validation]\n",
    "files_valid_label = get_files_in_folder(conf.label_validation)\n",
    "_, idcs = sample_list(files_valid_label, n_samples=conf.max_samples_validation)\n",
    "files_valid_input = [np.take(f, idcs) for f in files_valid_input]\n",
    "files_valid_label = np.take(files_valid_label, idcs)\n",
    "print(f\"Found {len(files_valid_label)} validation samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataset pipeline parsing functions\n",
    "def parse_sample(input_files, label_file):\n",
    "    # parse and process input images\n",
    "    inputs = []\n",
    "    for inp in input_files:\n",
    "        inp = load_image_op(inp)\n",
    "        inp = resize_image_op(inp, image_shape_original_input, conf.image_shape, interpolation=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        inp = one_hot_encode_image_op(inp, conf.one_hot_palette_input)\n",
    "        inputs.append(inp)\n",
    "    inputs = inputs[0] if n_inputs == 1 else tuple(inputs)\n",
    "    # parse and process label image\n",
    "    label = load_image_op(label_file)\n",
    "    label = resize_image_op(label, image_shape_original_label, conf.image_shape, interpolation=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    label = one_hot_encode_image_op(label, conf.one_hot_palette_label)\n",
    "    return inputs, label\n",
    "\n",
    "# build training data pipeline\n",
    "dataTrain = tf.data.Dataset.from_tensor_slices((tuple(files_train_input), files_train_label))\n",
    "dataTrain = dataTrain.shuffle(buffer_size=conf.max_samples_training, reshuffle_each_iteration=True)\n",
    "dataTrain = dataTrain.map(parse_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataTrain = dataTrain.batch(conf.batch_size, drop_remainder=True)\n",
    "dataTrain = dataTrain.repeat(conf.epochs)\n",
    "dataTrain = dataTrain.prefetch(1)\n",
    "print(\"Built data pipeline for training\")\n",
    "\n",
    "# build validation data pipeline\n",
    "dataValid = tf.data.Dataset.from_tensor_slices((tuple(files_valid_input), files_valid_label))\n",
    "dataValid = dataValid.map(parse_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataValid = dataValid.batch(1)\n",
    "dataValid = dataValid.repeat(conf.epochs)\n",
    "dataValid = dataValid.prefetch(1)\n",
    "print(\"Built data pipeline for validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_meshgrid(x, y):\n",
    "    return tf.meshgrid(x, y)\n",
    "\n",
    "\n",
    "def K_linspace(start, stop, num):\n",
    "    return tf.linspace(start, stop, num)\n",
    "\n",
    "\n",
    "class BilinearInterpolation(Layer):\n",
    "    \"\"\"Performs bilinear interpolation as a keras layer\n",
    "    References\n",
    "    ----------\n",
    "    [1]  Spatial Transformer Networks, Max Jaderberg, et al.\n",
    "    [2]  https://github.com/skaae/transformer_network\n",
    "    [3]  https://github.com/EderSantana/seya\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size, **kwargs):\n",
    "        self.output_size = output_size\n",
    "        super(BilinearInterpolation, self).__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'output_size': self.output_size,\n",
    "        }\n",
    "\n",
    "    def compute_output_shape(self, input_shapes):\n",
    "        height, width = self.output_size\n",
    "        num_channels = input_shapes[0][-1]\n",
    "        return (None, height, width, num_channels)\n",
    "\n",
    "    def call(self, tensors, mask=None):\n",
    "        X, transformation = tensors\n",
    "        output = self._transform(X, transformation, self.output_size)\n",
    "        return output\n",
    "\n",
    "    def _interpolate(self, image, sampled_grids, output_size):\n",
    "\n",
    "        batch_size = K.shape(image)[0]\n",
    "        height = K.shape(image)[1]\n",
    "        width = K.shape(image)[2]\n",
    "        num_channels = K.shape(image)[3]\n",
    "\n",
    "        x = K.cast(K.flatten(sampled_grids[:, 0:1, :]), dtype='float32')\n",
    "        y = K.cast(K.flatten(sampled_grids[:, 1:2, :]), dtype='float32')\n",
    "\n",
    "        x = .5 * (x + 1.0) * K.cast(width, dtype='float32')\n",
    "        y = .5 * (y + 1.0) * K.cast(height, dtype='float32')\n",
    "\n",
    "        x0 = K.cast(x, 'int32')\n",
    "        x1 = x0 + 1\n",
    "        y0 = K.cast(y, 'int32')\n",
    "        y1 = y0 + 1\n",
    "\n",
    "        max_x = int(K.int_shape(image)[2] - 1)\n",
    "        max_y = int(K.int_shape(image)[1] - 1)\n",
    "\n",
    "        x0 = K.clip(x0, 0, max_x)\n",
    "        x1 = K.clip(x1, 0, max_x)\n",
    "        y0 = K.clip(y0, 0, max_y)\n",
    "        y1 = K.clip(y1, 0, max_y)\n",
    "\n",
    "        pixels_batch = K.arange(0, batch_size) * (height * width)\n",
    "        pixels_batch = K.expand_dims(pixels_batch, axis=-1)\n",
    "        flat_output_size = output_size[0] * output_size[1]\n",
    "        base = K.repeat_elements(pixels_batch, flat_output_size, axis=1)\n",
    "        base = K.flatten(base)\n",
    "\n",
    "        # base_y0 = base + (y0 * width)\n",
    "        base_y0 = y0 * width\n",
    "        base_y0 = base + base_y0\n",
    "        # base_y1 = base + (y1 * width)\n",
    "        base_y1 = y1 * width\n",
    "        base_y1 = base_y1 + base\n",
    "\n",
    "        indices_a = base_y0 + x0\n",
    "        indices_b = base_y1 + x0\n",
    "        indices_c = base_y0 + x1\n",
    "        indices_d = base_y1 + x1\n",
    "\n",
    "        flat_image = K.reshape(image, shape=(-1, num_channels))\n",
    "        flat_image = K.cast(flat_image, dtype='float32')\n",
    "        pixel_values_a = K.gather(flat_image, indices_a)\n",
    "        pixel_values_b = K.gather(flat_image, indices_b)\n",
    "        pixel_values_c = K.gather(flat_image, indices_c)\n",
    "        pixel_values_d = K.gather(flat_image, indices_d)\n",
    "\n",
    "        x0 = K.cast(x0, 'float32')\n",
    "        x1 = K.cast(x1, 'float32')\n",
    "        y0 = K.cast(y0, 'float32')\n",
    "        y1 = K.cast(y1, 'float32')\n",
    "\n",
    "        area_a = K.expand_dims(((x1 - x) * (y1 - y)), 1)\n",
    "        area_b = K.expand_dims(((x1 - x) * (y - y0)), 1)\n",
    "        area_c = K.expand_dims(((x - x0) * (y1 - y)), 1)\n",
    "        area_d = K.expand_dims(((x - x0) * (y - y0)), 1)\n",
    "\n",
    "        values_a = area_a * pixel_values_a\n",
    "        values_b = area_b * pixel_values_b\n",
    "        values_c = area_c * pixel_values_c\n",
    "        values_d = area_d * pixel_values_d\n",
    "        return values_a + values_b + values_c + values_d\n",
    "\n",
    "    def _make_regular_grids(self, batch_size, height, width):\n",
    "        # making a single regular grid\n",
    "        x_linspace = K_linspace(-1., 1., width)\n",
    "        y_linspace = K_linspace(-1., 1., height)\n",
    "        x_coordinates, y_coordinates = K_meshgrid(x_linspace, y_linspace)\n",
    "        x_coordinates = K.flatten(x_coordinates)\n",
    "        y_coordinates = K.flatten(y_coordinates)\n",
    "        ones = K.ones_like(x_coordinates)\n",
    "        grid = K.concatenate([x_coordinates, y_coordinates, ones], 0)\n",
    "\n",
    "        # repeating grids for each batch\n",
    "        grid = K.flatten(grid)\n",
    "        grids = K.tile(grid, K.stack([batch_size]))\n",
    "        return K.reshape(grids, (batch_size, 3, height * width))\n",
    "\n",
    "    def _transform(self, X, transformation, output_size):\n",
    "        \n",
    "        batch_size, num_channels = K.shape(X)[0], X.shape[3]\n",
    "\n",
    "        transformation = K.reshape(transformation, shape=(batch_size, 3, 3))\n",
    "\n",
    "        # create regular grid (-1,1)x(-1,1)\n",
    "        regular_grids = self._make_regular_grids(batch_size, *output_size)\n",
    "\n",
    "        # transform regular grid\n",
    "        sampled_grids = K.batch_dot(transformation, regular_grids)\n",
    "\n",
    "        # homogeneous coords: divide by 3rd component w\n",
    "        w = tf.math.reciprocal(sampled_grids[:, 2, :])\n",
    "        w = tf.reshape(w, (batch_size, 1, w.shape[-1]))\n",
    "        w = tf.tile(w, [1, 2, 1])\n",
    "        sampled_grids = Multiply()([sampled_grids[:, 0:2, :], w])\n",
    "\n",
    "        # interpolate image\n",
    "        interpolated_image = self._interpolate(X, sampled_grids, output_size)\n",
    "        new_shape = (batch_size, output_size[0], output_size[1], num_channels)\n",
    "        interpolated_image = K.reshape(interpolated_image, new_shape)\n",
    "\n",
    "        return interpolated_image\n",
    "\n",
    "\n",
    "def STN_example_model(input_shape=(60, 60, 1), sampling_size=(30, 30), num_classes=10):\n",
    "\n",
    "    image = Input(shape=input_shape)\n",
    "    locnet = MaxPooling2D(pool_size=(2, 2))(image)\n",
    "    locnet = Conv2D(20, (5, 5))(locnet)\n",
    "    locnet = MaxPooling2D(pool_size=(2, 2))(locnet)\n",
    "    locnet = Conv2D(20, (5, 5))(locnet)\n",
    "    locnet = Flatten()(locnet)\n",
    "    locnet = Dense(50)(locnet)\n",
    "    locnet = Activation('relu')(locnet)\n",
    "    def get_initial_weights(output_size):\n",
    "        b = np.zeros((2, 3), dtype='float32')\n",
    "        b[0, 0] = 1\n",
    "        b[1, 1] = 1\n",
    "        W = np.zeros((output_size, 6), dtype='float32')\n",
    "        weights = [W, b.flatten()]\n",
    "        return weights\n",
    "    weights = get_initial_weights(50)\n",
    "    locnet = Dense(6, weights=weights)(locnet)\n",
    "    x = BilinearInterpolation(sampling_size)([image, locnet])\n",
    "    x = Conv2D(32, (3, 3), padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(32, (3, 3))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dense(num_classes)(x)\n",
    "    x = Activation('softmax')(x)\n",
    "    return Model(inputs=image, outputs=x)\n",
    "\n",
    "\n",
    "def _spatial_transformer(input, output_shape, theta_init=np.eye(3), theta_const=False, loc_downsample=3, dense_units=20, filters=16, kernel_size=(3,3), activation=tf.nn.relu, dense_reg=0.0):\n",
    "\n",
    "    theta_init = theta_init.flatten().astype(np.float32)\n",
    "\n",
    "    if not theta_const:\n",
    "\n",
    "        t = input\n",
    "\n",
    "        # initialize transform to identity\n",
    "        init_weights = [np.zeros((dense_units, 9), dtype=np.float32), theta_init]\n",
    "\n",
    "        # localization network\n",
    "        for d in range(loc_downsample):\n",
    "            t = Conv2D(filters=filters, kernel_size=kernel_size, padding=\"same\", activation=activation)(t)\n",
    "            t = MaxPooling2D(pool_size=(2,2), padding=\"same\")(t)\n",
    "        t = Flatten()(t)\n",
    "        t = Dense(dense_units)(t)\n",
    "\n",
    "        k_reg = tf.keras.regularizers.l2(dense_reg) if dense_reg > 0 else None\n",
    "        b_reg = tf.keras.regularizers.l2(dense_reg) if dense_reg > 0 else None\n",
    "        theta = Dense(9, weights=init_weights, kernel_regularizer=k_reg, bias_regularizer=b_reg)(t) # transformation parameters\n",
    "\n",
    "    else:\n",
    "\n",
    "        theta = tf.tile(theta_init, tf.shape(input)[0:1])\n",
    "\n",
    "    # transform feature map\n",
    "    output = BilinearInterpolation(output_shape)([input, theta])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def SpatialTransformer(input_shape, output_shape, theta_init=np.eye(3), theta_const=False, loc_downsample=3, dense_units=20, filters=16, kernel_size=(3,3), activation=tf.nn.relu, dense_reg=0.0):\n",
    "\n",
    "    input = Input(input_shape)\n",
    "    output = _spatial_transformer(input, output_shape[0:2], theta_init, theta_const, loc_downsample, dense_units, filters, kernel_size, activation, dense_reg)\n",
    "\n",
    "    return Model(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input, udepth=3, filters1=8, kernel_size=(3,3), activation=tf.nn.relu, batch_norm=True, dropout=0.1):\n",
    "\n",
    "    t = input\n",
    "    encoder_layers = udepth * [None]\n",
    "\n",
    "    # common parameters\n",
    "    pool_size = (2,2)\n",
    "    padding = \"same\"\n",
    "\n",
    "    # layer creation with successive pooling\n",
    "    for d in range(udepth):\n",
    "        filters = (2**d) * filters1\n",
    "        t = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation=activation)(t)\n",
    "        t = BatchNormalization()(t) if batch_norm else t\n",
    "        t = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation=activation)(t)\n",
    "        t = encoder_layers[d] = BatchNormalization()(t) if batch_norm else t\n",
    "        if d < (udepth - 1):\n",
    "            t = MaxPooling2D(pool_size=pool_size, padding=padding)(t)\n",
    "            t = Dropout(rate=dropout)(t) if dropout > 0 else t\n",
    "\n",
    "    return encoder_layers\n",
    "\n",
    "\n",
    "def joiner(list_of_encoder_layers, thetas, filters1=8, kernel_size=(3,3), activation=tf.nn.relu, batch_norm=True, double_skip_connection=False):\n",
    "\n",
    "    n_inputs = len(list_of_encoder_layers)\n",
    "    udepth = len(list_of_encoder_layers[0])\n",
    "    encoder_layers = udepth * [None]\n",
    "\n",
    "    for d in range(udepth):\n",
    "        filters = (2**d) * filters1\n",
    "        shape = list_of_encoder_layers[0][d].shape[1:]\n",
    "\n",
    "        warped_maps = []\n",
    "        for i in range(n_inputs): # use Spatial Transformer with constant homography transformation before concatenating\n",
    "            # Problem w/ trainable theta: regularization necessary, huge loss, always went to loss=nan\n",
    "            t = SpatialTransformer(shape, shape, theta_init=thetas[i], theta_const=True)(list_of_encoder_layers[i][d])\n",
    "            warped_maps.append(t)\n",
    "        t = Concatenate()(warped_maps) if n_inputs > 1 else warped_maps[0]\n",
    "        t = Conv2D(filters=filters, kernel_size=kernel_size, padding=\"same\", activation=activation)(t)\n",
    "        t = BatchNormalization()(t) if batch_norm else t\n",
    "        t = Conv2D(filters=filters, kernel_size=kernel_size, padding=\"same\", activation=activation)(t)\n",
    "        t = warped = BatchNormalization()(t) if batch_norm else t\n",
    "\n",
    "        if not double_skip_connection:\n",
    "\n",
    "            t = encoder_layers[d] = warped\n",
    "\n",
    "        else:\n",
    "\n",
    "            nonwarped_maps = []\n",
    "            for i in range(n_inputs): # also concat non-warped maps\n",
    "                t = list_of_encoder_layers[i][d]\n",
    "                nonwarped_maps.append(t)\n",
    "            t = Concatenate()(nonwarped_maps) if n_inputs > 1 else nonwarped_maps[0]\n",
    "            t = Conv2D(filters=filters, kernel_size=kernel_size, padding=\"same\", activation=activation)(t)\n",
    "            t = BatchNormalization()(t) if batch_norm else t\n",
    "            t = Conv2D(filters=filters, kernel_size=kernel_size, padding=\"same\", activation=activation)(t)\n",
    "            t = nonwarped = BatchNormalization()(t) if batch_norm else t\n",
    "\n",
    "            # concat both\n",
    "            t = Concatenate()([warped, nonwarped])\n",
    "            t = Conv2D(filters=filters, kernel_size=kernel_size, padding=\"same\", activation=activation)(t)\n",
    "            t = BatchNormalization()(t) if batch_norm else t\n",
    "            t = Conv2D(filters=filters, kernel_size=kernel_size, padding=\"same\", activation=activation)(t)\n",
    "            t = encoder_layers[d] = BatchNormalization()(t) if batch_norm else t\n",
    "\n",
    "    return encoder_layers\n",
    "\n",
    "\n",
    "def decoder(encoder_layers, udepth=3, filters1=8, kernel_size=(3,3), activation=tf.nn.relu, batch_norm=True, dropout=0.1):\n",
    "\n",
    "    # start at lowest encoder layer\n",
    "    t = encoder_layers[udepth-1]\n",
    "\n",
    "    # common parameters\n",
    "    strides = (2,2)\n",
    "    padding = \"same\"\n",
    "\n",
    "    # layer expansion symmetric to encoder\n",
    "    for d in reversed(range(udepth-1)):\n",
    "        filters = (2**d) * filters1\n",
    "        t = Conv2DTranspose(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding)(t)\n",
    "        t = Concatenate()([encoder_layers[d], t])\n",
    "        t = Dropout(rate=dropout)(t) if dropout > 0 else t\n",
    "        t = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation=activation)(t)\n",
    "        t = BatchNormalization()(t) if batch_norm else t\n",
    "        t = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation=activation)(t)\n",
    "        t = BatchNormalization()(t) if batch_norm else t\n",
    "\n",
    "    return t\n",
    "\n",
    "\n",
    "def get_network(input_shape, n_output_channels, n_inputs, thetas, \n",
    "                udepth = 5, \n",
    "                filters1 = 16, \n",
    "                kernel_size = (3,3), \n",
    "                activation = tf.nn.relu, \n",
    "                batch_norm = True, \n",
    "                dropout = 0.1,\n",
    "                double_skip_connection = False):\n",
    "\n",
    "    # build inputs\n",
    "    inputs = [Input(input_shape) for i in range(n_inputs)]\n",
    "\n",
    "    # encode all inputs separately\n",
    "    list_of_encoder_layers = []\n",
    "    for i in inputs:\n",
    "        encoder_layers = encoder(i, udepth, filters1, kernel_size, activation, batch_norm, dropout)\n",
    "        list_of_encoder_layers.append(encoder_layers)\n",
    "\n",
    "    # fuse encodings of all inputs at all layers\n",
    "    encoder_layers = joiner(list_of_encoder_layers, thetas, filters1, kernel_size, activation, batch_norm, double_skip_connection)\n",
    "\n",
    "    # decode from bottom to top layer\n",
    "    reconstruction = decoder(encoder_layers, udepth, filters1, kernel_size, activation, batch_norm, dropout)\n",
    "\n",
    "    # build final prediction layer\n",
    "    prediction = Conv2D(filters=n_output_channels, kernel_size=kernel_size, padding=\"same\", activation=activation)(reconstruction)\n",
    "    prediction = Activation(\"softmax\")(prediction)\n",
    "\n",
    "    return Model(inputs, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_network((conf.image_shape[0], conf.image_shape[1], n_classes_input), n_classes_label, n_inputs=n_inputs, thetas=H)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=conf.learning_rate)\n",
    "loss = weighted_categorical_crossentropy(conf.loss_weights)\n",
    "\n",
    "metrics = [tf.keras.metrics.CategoricalAccuracy(), MeanIoUWithOneHotLabels(num_classes=n_classes_label)]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "print(f\"Compiled model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output directories\n",
    "model_output_dir = os.path.join(conf.output_dir, now_time)\n",
    "tensorboard_dir = os.path.join(model_output_dir, \"TensorBoard\")\n",
    "checkpoint_dir  = os.path.join(model_output_dir, \"Checkpoints\")\n",
    "if not os.path.exists(tensorboard_dir):\n",
    "    os.makedirs(tensorboard_dir)\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "\n",
    "# create callbacks to be called after each epoch\n",
    "n_batches_train = len(files_train_label) // conf.batch_size\n",
    "n_batches_valid = len(files_valid_label)\n",
    "tensorboard_cb      = tf.keras.callbacks.TensorBoard(tensorboard_dir, update_freq=\"epoch\", profile_batch=0)\n",
    "checkpoint_cb       = tf.keras.callbacks.ModelCheckpoint(os.path.join(checkpoint_dir, \"e{epoch:03d}_weights.hdf5\"), save_freq=n_batches_train*conf.save_interval, save_weights_only=True)\n",
    "best_checkpoint_cb  = tf.keras.callbacks.ModelCheckpoint(os.path.join(checkpoint_dir, \"best_weights.hdf5\"), save_best_only=True, monitor=\"val_mean_io_u_with_one_hot_labels\", mode=\"max\", save_weights_only=True)\n",
    "early_stopping_cb   = tf.keras.callbacks.EarlyStopping(monitor=\"val_mean_io_u_with_one_hot_labels\", mode=\"max\", patience=conf.early_stopping_patience, verbose=1)\n",
    "callbacks = [tensorboard_cb, checkpoint_cb, best_checkpoint_cb, early_stopping_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retraining for better performance\n",
    "# These weights are from a previous training\n",
    "model.load_weights('../input/temporary/best_weights.hdf5')\n",
    "print(f\"Reloaded weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "model.fit(dataTrain,\n",
    "          epochs=conf.epochs, steps_per_epoch=n_batches_train,\n",
    "          validation_data=dataValid, validation_freq=1, validation_steps=n_batches_valid,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.model_weights = os.path.join(checkpoint_dir, 'best_weights.hdf5') \n",
    "conf.prediction_dir = os.path.join(model_output_dir, 'Predictions')\n",
    "if not os.path.exists(conf.prediction_dir):\n",
    "    os.makedirs(conf.prediction_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max_samples_testing samples\n",
    "files_input = [get_files_in_folder(folder) for folder in conf.input_testing]\n",
    "files_label = get_files_in_folder(conf.label_testing)\n",
    "_, idcs = sample_list(files_input[0], n_samples=conf.max_samples_testing)\n",
    "files_input = [np.take(f, idcs) for f in files_input]\n",
    "files_label = np.take(files_label, idcs)\n",
    "\n",
    "n_inputs = len(conf.input_testing)\n",
    "n_samples = len(files_input[0])\n",
    "image_shape_original = load_image(files_input[0][0]).shape[0:2]\n",
    "image_shape_original_label = load_image(files_label[0]).shape[0:2]\n",
    "\n",
    "print(f\"Found {n_samples} samples\")\n",
    "\n",
    "# build data parsing function\n",
    "def parse_sample(input_files):\n",
    "    # parse and process input images\n",
    "    inputs = []\n",
    "    for inp in input_files:\n",
    "        inp = load_image_op(inp)\n",
    "        inp = resize_image_op(inp, image_shape_original, conf.image_shape, interpolation=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        inp = one_hot_encode_image_op(inp, conf.one_hot_palette_input)\n",
    "        inputs.append(inp)\n",
    "    inputs = inputs[0] if n_inputs == 1 else tuple(inputs)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\"Front\", \"Rear\", \"Left\", \"Right\", \"BEV\", \"Actual BEV\"]\n",
    "\n",
    "# run predictions\n",
    "print(f\"Running predictions and writing to {conf.prediction_dir} ...\")\n",
    "for k in random.sample(range(n_samples), 10):\n",
    "\n",
    "    input_files = [files_input[i][k] for i in range(n_inputs)]\n",
    "    label_file = files_label[k]\n",
    "\n",
    "    # load sample\n",
    "    inputs = parse_sample(input_files)\n",
    "\n",
    "    # add batch dim\n",
    "    if n_inputs > 1:\n",
    "        inputs = [np.expand_dims(i, axis=0) for i in inputs]\n",
    "    else:\n",
    "        inputs = np.expand_dims(inputs, axis=0)\n",
    "\n",
    "    # run prediction\n",
    "    prediction = model.predict(inputs).squeeze()\n",
    "\n",
    "    # convert to output image\n",
    "    prediction = one_hot_decode_image(prediction, conf.one_hot_palette_label)\n",
    "    \n",
    "    plt.figure(figsize=(25,7))\n",
    "    plt.tight_layout()\n",
    "    for i in range(n_inputs + 2):\n",
    "        ax = plt.subplot(2, 4, i + 1)\n",
    "        \n",
    "        if i == n_inputs:\n",
    "            img = prediction\n",
    "        elif i == n_inputs + 1:\n",
    "            img = load_image(label_file)\n",
    "        else:\n",
    "            img = load_image(input_files[i])\n",
    "            \n",
    "        plt.axis('off')\n",
    "        plt.imshow(img)\n",
    "        \n",
    "    plt.show()\n",
    "            \n",
    "    # write to disk\n",
    "    output_file = os.path.join(conf.prediction_dir, os.path.basename(files_input[0][k]))\n",
    "    cv2.imwrite(output_file, cv2.cvtColor(prediction, cv2.COLOR_RGB2BGR))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
